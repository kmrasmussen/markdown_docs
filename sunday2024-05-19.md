# Sunday 2024-05-19

Understanding what human happiness is very difficult.
It seems clear that motivation is a factor.
I believe motivation is usefully analysed from the perspective of hierarchical task structures. However such a view has to be complemented with viewing human motivation and execution from a perspective where a strong hierarchical task structure is precisely missing because it is subsumed into a general feeling of what needs to be done which can maybe be usefully analysed using the terminology of value functions from reinforcement learning.

## Ideas about non-structured motivation
We are born with relatively innate biological neural mechanisms that assign reward to specific states and actions. Given our cognitive capabilities and planning abilities we are able to imagine certain states of the world and our life quite far ahead. The innate biological neural mechanisms allow us to asign value to these states and compare the value of different states of affairs.

Through our cognitive learning abilities as we grow and experience the world our system forms new models of what is valuable that is not directly innate. Still it is to be expected that in many cases, what these models value is still heavily influenced by the innate value evaluations: They are contextualizations of the innate value evalutations. As an example, there is most likely an innate valuing of companionship and being integrated in a social structure. A person might experience that playing music in a band is a way of connecting with other people, gradually changes to valuations will occur where playing music in a band is viewed as immediately rewarding, and not only the act but the thought of it.

The neural learning of derived valuing is important for acting in the world. It is related to the credit assignment problem of reinforcment learning. In order to succesfully learn RL policies reward needs to be non-sparse. This is not to say that the innate biological valuing is especially sparse; however, it does not allow for intelligent planning at a higher level. Biological valuing evaluates states according to things such as pain, hunger, sexual gratification etc. In order for human being to be adaptive in the human environment it needs to be able to do things and reach states of the world that are not easily reachable by following the most direct route of immediate biologically valued reward. Sacrificing immediate reward. It seems that learned valuing allows for this in a different way: The reward system couples to the learned features available from the perceptual system.

## Ideas about structured motivation
We have bounded short-term memory. It may or may not be that short-term memory is a natural kind of human psychology, I have not read enough about studies but I get the impression that people think there are probably different circuits and that it is not entirely a continuum. It would be tempting to think that it is a continuum though.

The short-term memory I am thinking about here is not exactly working memory, but in some sense there must be some limitations on how complex the hierarchical structures of action we need to perform can be active in memory. This is why many of us use to-do lists and other externalized tools to aid our memory in solving a task.

* If a task is too big and has not been broken down, you will lose motivation. You will feel tired. You have to break it down.
* If you break a task down depth first and end up thinking about some little thing you will lose motivation. You will feel frustrated. You have to then go back up.
* You somehow have to try to always maintain the right level of generality of what you are thinking about and working on. You have to stay clear on a large-scale task but also work on something concrete that relates to the larger task.

## Notes on GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints
[https://arxiv.org/pdf/2305.13245](https://arxiv.org/pdf/2305.13245)

* Normally, for each attention head indexed by $h$, we have unique affine "query heads" $Q_h : \mathbb{R}^d \to \mathbb{R}^{d_{att}}$ and key and value heads $K_h : \mathbb{R}^d \to \mathbb{R}^{d_{att}}$, $V_h : \mathbb{R}^d \to \mathbb{R}^{d}$. These take the the current residual stream vector $r_t \in \mathcal{R}^d$ at at each position $t$ and apply the affine functions to get $q_t = Q_h(r_t)$, $k_t = K_h(r_t)$, and $v_t = V_h(r_t)$ to perform the computation of the attention head.
* Multi-query attention: In multi-query attention, for each head $h$ we use the same shared key and value heads, $K_h = K$ and $V_h = V$. This clearly requires less parameters. A reason for doing this is that with current setups, we have limited memory on the GPU. Therefore we need to load both the weights for $Q_h$, $K_h$, and $V_h$ (which are usually affine functions, so a "projection" matrix and maybe a bias vector) but also the KV-cache: Usually to speed up computation at inference time, we are caching the computed $k_t$ and $v_t$ from the previous forward pass, so that when generating the next token we do not need to compute it again, forming a "KV-cache". If we have long sequences of length $n$, the $n$ $k_t$ and $v_t$ vectors take up a lot of space. Since in the normal case, each head has its own $k_t$ and $v_t$ by sharing the same key and value heads, the $k_t$ and $v_t$ are shared between heads and the KV-cache does not need to be as big. If we have $H$ heads, it is reduced by a factor $H$. This reduces the memory bandwidth of loading the KV-cache in.
* MQA leads to worse performance which makes sense since there is usually a trade-off between model size and performance. MQA also leads to training instability.
* The paper has two parts: It shows that a model that is trained the normal way can be "uptrained" to use MQA using only little compute. It also shows GQA, where groups of query heads will share the same key and value head, so it is a kind of middle-ground.
* Uptraining, which means conversion of a normal model to MQA, means that you have to figure out how the shared key and value head will be constructed. Consider the key case, we have for the layer $H$ heads, giving $H$ key heads. There is no bias vector, or at least is it not mentioned, so we have $H$ matrices, $K_1, ..., K_H \in \mathbb{R}^{d \times d_{head}}$. They construct the new $K$ by max-pooling of the matrices. This means that index $i,j$ in $K$ will be
$$
K[i,j] = \max_h(K_h[i,j])
$$
Then, to adapt the model to the changes they start pre-training process: Instead of pre-training from random initialized model, they treat this constructed model as the initialization and use the original training recipe, but stop already after a small amount of steps.
* GQA is a generalization of MQA, so we take the heads of the original model and partition them in groups, so group with index $g$ and group size $m$ would have $Q^g_1, ..., Q^h_m$ and likewise $K^g_1, ..., K^g_m$. Then for the group we would compute a new key head $K^g$ by max-pooling those key heads, and likewise for $V^g$.
* They say that usually as we scale to bigger models we also scale the number of heads per layer. Therefore as we go to bigger models MQA would be too big of a cut and with GQA we are able to make it more proportional.
* Assume a fixed sequence length $n$. They say that as we scale to bigger models we scale the model dimension $d$, as we scale $d$ the forward pass total FLOP and parameter count scale as $d^2$. The KV-cache only scales with $d$ (since the KV-cache has $H$ heads, and for each of the $n$ tokens keeps $k_t$ and $v_t$ each vectors of length $d$, the KV-cache has size $2nHd$, ignoring batch.). Why does parameter count scale as $d^2$. Parameter count is dominated by the MLPs, usally they use a branching factor of 4. The MLP has two matrices, in and out, of shapes $d \times 4d$ and $4d \times d$, giving $8d^2$ parameters. The FLOPs used when applying the MLP on a single residual stream vector is $d^2$ because you are making $O(d)$ dot-products each taking $O(d)$ FLOP.
* They also mention something about how GQA reduces waste from sharding but I don't know much about sharding.
* I wonder how GQA affect very long context cases. Seems like they didn't look much at it.

## Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention
[https://arxiv.org/pdf/2404.07143](https://arxiv.org/pdf/2404.07143)

# Coding

## Making a clipboard subscriber
Goal: I just want a simple script that can subscribe to a redis channel and set the textclipboard
accordingly.
Reason (parent): I want to have a good setup where I can always work just in the cloud,
sshing to my ec2 and so on. I want to run cb.py there, so I can develop cb.py into something
powerful, but using the clipboard is an important part.
Additional: It should also not take a long time, but maybe it will if it is difficult to get python running on the thinkpad.
Solution:
Okay, got it working and also with cb.py
```python

import redis
import pyperclip

# Configure Redis connection
redis_host = '123423542345'  # Replace with your Redis host
redis_password = '13451354234'
redis_port = 6379       # Replace with your Redis port
redis_channel = 'clipboard'

# Connect to Redis
r = redis.Redis(host=redis_host, port=redis_port, password=redis_password, decode_responses=True)

# Subscribe to the clipboard channel
pubsub = r.pubsub()
pubsub.psubscribe(redis_channel)

# Keep listening for messages
for message in pubsub.listen():
    if message['type'] == 'pmessage':
        # Get the channel and message
        channel = message['channel']
        data = message['data']

        # Set the clipboard
        pyperclip.copy(data)

        print(f"Clipboard updated with: {data}")

# Keep the script running
while True:
    pass
```

# cb.py ideas

* There should be some kind of awareness of the shell current work dir
- first we need to make some kind of aliasing work to even be able to execute cb.py at different directories. Ok, this already works.
* the GUI web interface will be easier to make once cb.py itself works well
* There is something about whether cb.py should be inside or outside a shell
* I now have it so that cb.py maintains a set of open files, and will prepend the contents to the prompt.
* Next some basic file edits.
* Okay, I added basic tool usage now, works with a simple tool called `singlelinedit` - quite magical to see it work.
* Next things
- Need to make it so that it does not apply the tool actions automatically but asks the user
- Need to make linereplaceedit(filename, linestart, lineend, newcontent) and insertafterdit(filename, linebefore, content)
- Need to think more about processes, maybe it is possible to have some kind of proxy launcher that will execute and write to a file, so you not only have open files in cbpy but also open processes, where the tail of stdout/stderr and stdin are added to the preprompt.
* Now the basic editing functions replacement and insertion work
* Next step is the shell, I will need to rework some things to make it work well.
- I need a agentshell-server.py that stays alive and manages the different active tabs and gives access to input and output to the processes.
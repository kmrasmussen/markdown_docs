password: 1234

# Notes on Saturday May 25 2024

* Private docs: In mdflaskpy, it would be nice if we just can have that if the first line of a markdown document is `password: 1234` then it is only accessible with password. Even though it is not safe, for a start it would be fine to just use the get parameter `access_key`.
* I looked a bit on fmri registration. I think it would be interesting to try to have a detailed understanding of the preprocessing pipelines, demystifying everything. In general I think it is possible to understand everything much more than you think. You just need patience and willingness to take time aside from your main goal to learn things, for example I have spent a couple of hours trying to build ANTs, and there I learned a bit about build. I was maybe a bit slow and unprincipled but whatever.
* The idea is that the registration is a concrete part of the preprocessing pipeline and it seems from reading a bit about it and talking to GPT that it is a part of the pipeline that could in principle be accelerated on GPUs so it could be a fun project to try to make something that behaves a lot like antRegistration but which is implemented in pytorch.
* Ideas are super important for motivation, so are visions and ideals.
* I don't know when ideas come, but you need to not be so strongly focused on an existing goal that you ignore ideas about other things, still you need to be actively engaged in tasks and notice some kinds of things that could be done better or in a different way. You also need to relax. And smile a bit maybe :)))
* I notice I have this hurried feeling - time has passed and it is already 15.30. I feel lonely.
* I will practice a bit of Vimified.

## Notes on AI hardware talk notes
* looking at h100
* stepped down to 48v
* hbm: high bandwith memory
* sm: streaming multiprocessors - fundamental building blocks.
- each sm has multiple CUDA cores, registers and shared memory
- an sm responsible for executing parallel thread blocks, scheduling threads for execution
* l2 cache broken into two, cable between them
* 3 HBMs on each side, connected to memory controllers
* inside single sm
- the whole sm has a shared 256 KB L1 data cache or used as shared memory, for communication within sm
- instruction cache on top
- divided into 4. In each section INT32 units, FP32, FP64 and then 1 Tensor Core 4th gen
- register file has the fastest memory 16K 32-bit registers
- L0 instruction cache
- deep memory hierarchy - makes physical sense, there is locality, downside there is only so much space that is close by
- most inefficiency (low utilisation) comes from bad way of using memory, this is the key of programming GPUs, e.g. flash attention is primarily a way of having it very low in hierarchy and not go too far back, using the mem hierarchy to full extent
- less than 10% of die is used for actual matmuls rest is for making it possible
* without tensor cores around 100 Tflops, with 1000 teraflops, 30x difference in practice

## Building intuition
* H100 has 132 SMs, 1K Tflops fp16, 3 TBps bandwith off-chip
* AMD spearheaded HBM - thought gaming gpus would be bottlenecked by bandwidth
* microbenchmark: 12 tbps from L2
* 33 tbps from l1
* roofline plot: arithmetic intensity, operations we perform per byte we move. for low intensity we have less operations per byte we get and therefore we cannot use all the flops available, so we are bottlenecked by memory bandwidth. When we have high intensity, we have many operations per byte we get so compute is the bottleneck. We are compute bound instead of memory bound. We want to be compute bound because then we are utilising the flops.
* FP16 square matrix multiply: nxn @ nxn
- How many flops for this? The result is nxn, for each entry we need a dot-product. Each dot product is n multiplications + n-1 additions so 2n flop. So 2n^3 flop total.
- How much memory? Depends on how you think about it. You need 2nxn for the input matrices and nxn for the output matrix. At the algorithmic level you don't need more memory but in practice you might need because of cache hierarchy? Since we have FP16 we have 2 bytes per, so 3n^2 entries take up 6n^2 bytes.
* When we have ~2n^3 flop needed and ~6n^2 memory we compute intensity as compute/memory = 2n^3/6n^2 = n/3
* Roofline plot shots that roughly at arithmetic intensity 10^3 (x-axis) we start hitting the roof on flops (y-axis) meaning we enter compute-bound regime. So roughly we need to do such matmuls with size n=1000 to be compute bound and thus make best use of the compute
* this means that your architectures should also not things that deviate much from big matmuls

### Hardware vs software people think about computing
* software people: I am counting number of instructions, want few operations
* hardware: logic is cheap, it is movement and memory that is expensive
* chip has multiple layers. When we say it has 100b transistors this is only the bottom layer. The above layers are for communication and power delivery, the lowest levels are for local distance
* power consumption of chip rule: P = alpha * capacitance * voltage^2 * frequency
* cmos (complmentary metal oxide semiconductor): moving charge around to represent 1s and 0s
* tiny wires have tiny capacitance big wires have much capacitance

### Systolic array
* main idea: not move data a lot when doing matmul
* PE: does the multiply add used in matmul dotproducts
* Feed A entries on left side, parallelogram, feed B entries on top parallelogram
* C entries come out on bottom and right side as a parallelogram

### Networking
* scale is important, thousands of chips connected
* tpus uses optical switches, torus structure
* broadcom sells the switches
* in general want to avoid going off the silicon but you have to at some point if you want to scale - then the network is the core problem
* tpu paper is almost almost about the torus structure more than on-silicon
* physical reasons for why communication stack should be different from compute

### Design
* High level synthesis: The idea is writing chips in a high-level description/language, but is risky, experienced low-level chipdesigner beats it. Also writing chip itself is least difficult
* Logic design (RTL) - specification of which wires are connected
* functional verification - 2/3 time is testing instead of designing
* Synthesis & optimization - once you have a design you can try to optimize it, equivalent etc
* physical design: where should they be on the chip - rules like no wires of this type can be closer than this
* verification and analysis - simulation 
* different companies make software for different parts of design

## Processes
* photolithography: we have a detailed design on the tapeout and we want to put it on the chip
* ion implantation, diffusion - I want to make a transistor, originally you would heat it up and have something diffuse into it, now ion implantation because it is more precise
* etching - you also need to take stuff off the chip 
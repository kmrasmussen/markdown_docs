## Notes on Code Perception

By code perception I mean the processing of code that can happen in the forward pass. I call it perception in the sense that it is immediate processing with FLOPs proportional to the number of tokens of code, as opposed to reasoning about the code through CoT. In order for the model to reason about code through CoT, it will have to rely on code perception and the perception of past CoT steps. I will only discuss autoregressive/causal/decoder-only transformers that have causal masks on the attention matrices.

## Syntax

One aspect of code perception would be syntax. While it is not necessarily the case that the code perception involves a full syntactical parsing of the code, it seems that many cases of advanced reasoning would require or benefit from at least substantial syntactical parsing. Humans are also able to navigate in code without making a full syntactical parsing in their code perception.

In these models, the causality of attention influences representations of the code. If the code perception involves full syntactical parsing, then for a code block with format "if a then b else c", the representations of a, b, and c will have to map back to the if-token to represent their relative position in the AST. This means that the if statement token itself will carry very limited information. This approach to syntactical perception where nodes point back to parents does not pose major problems for the ability for models to have good perception of syntax.

## Semantics

Tigges2023 investigated representations of sentiment and found that for example the tokens for punctuation at the end of a sentence could store information about the sentiment of the entire sentence. Similarly, one might expect that languages with semicolon at the end of lines would use the tokens for semicolon to store information about the entire line, or (markup) languages with tokens marking the end of a block to do similar things. It is, of course, very characteristic of Python that many of these things are not present in the language, which then have resulted in code perception happening in a (slightly) different way.

## Lexical

Lexical analysis, where the code is parsed into tokens by the interpreter, is also an area one can investigate. To avoid confusion one will have to distinguish between interpreter-tokenizer and model-tokenizer. I had an example where even a frontier model, Claude Sonnet 3.5, hallucinated one part of an answer to a simple question about whether a small piece of code was Python2 or Python3, where it said the print-call was not using parentheses even though it was. While there is no way to know what exactly caused that, the difference between interpreter-tokenization and model-tokenization is something the model will have to work around in some cases, because the model-tokenization using BPE will often be more coarse-grained than the interpreter-tokenization.

The previous considerations about "x = F()" and also semicolons, mean that many investigations about code perception would include considerations of interpreter- and model-tokenization. The most immediate question one could consider here is: One could use the interpreter-tokenizer and get a list of strings and convert them to model-tokenizer tokens (I assume there are no cases where an interpreter-token-string cannot be mapped to one or more tokens in the model-tokenizer-vocabulary?). This could be given as input to the model, but the input would represent the code in a different way than it has normally seen during training, where the model-tokenizer is used in the standard BPE-way of finding the longest prefix in the vocabulary. Call this alternative approach interpreter-tokenization-usage as opposed to BPE-usage. I would predict that without further changes interpreter-tokenization-usage would result in a significant worsening of code perception. However, it seems that if one made other changes that allowed interpreter-tokenization-usage to work, such as using the approach using pre-training, it could in principle allow for a more "clean" code-perception: While large frontier models can probably in most cases (besides the Claude case I mentioned) work around the limitations of not having access to individual interpreter-tokens of the code, I could imagine interpreter-tokenization-usage would allow models to better learn the algorithms behind syntactical parsing and therefore increase the degree of syntactical parsing happening as part of code perception versus the usage of statistical features.

Since interpreter-tokenization is more fine-grained, interpreter-tokenization-usage would result in increased token count compared to BPE-usage. From a brief analysis of the CodeContests dataset, it seems interpreter-tokenization gives a median 1.22x increase in tokens relative to model-tokenization with the LLama3-tokenizer. Increased token count would mean that interpreter-tokenization would require more flops in forward pass which is less efficient, and sampling (generating new code from a prompt) would be less efficient because it requires more tokens to express the same code. Furthermore, it makes investigations tricky because increase token count under interpreter-tokenization-usage would increase the resources allocated to code perception, so it could be a kind of confounder when investigating if it is better. One would have to be aware of exactly what one is trying to improve (code perception/flops frontier?).

# Ideas for investigating

I want to consider what approaches one could take to investigating code perception. One could make benchmarks that require answering questions about the code. One could do mechanistic interpretability. One could combine the two. I am most interested in seeing if there are ways where one can noticeably improve code perception, ideally with limited training. It might be that very deliberately constructed pieces of code is needed for initial investigations but I think it is only interesting if it leads to some understanding that can be used to improve code perception in general.

- Predict syntax errors: Introduce syntactical errors into code (using some approach that could involve using LLMs or not) and ask the model if the code has errors or not. Can be benchmarking + interpretability. I think Anthropic looked a bit at this in one of the last superposition papers.
- Aspects of syntax analysis could be investigated using probing techniques, ideally causal ones. One can easily acquire ASTs for datasets of Python code and devise various annotations of the nodes of the ASTs, map the nodes back onto the code strings and onto the model-tokenized tokens and probe the residual streams for some of annotations of these nodes. From a general mechanistic interpretability perspective they represent an interesting class of features where we would have ground truth and the features would be computational. It might be possible to work off Atticus Geiger's causal abstraction analysis to investigate whether models implement algorithms for parsing code into ASTs.

